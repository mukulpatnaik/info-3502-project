{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reddit WallStreetBets Posts","metadata":{}},{"cell_type":"markdown","source":"**Importing the necessary Libraries**","metadata":{}},{"cell_type":"code","source":"# To prevent the annoyning Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\ncmap = sns.cm.mako_r\n\n%matplotlib inline\n\nimport re\nfrom nltk import word_tokenize, corpus\nfrom nltk.stem import PorterStemmer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nenglish_words = set(corpus.words.words())\n\nfrom IPython.core.display import HTML\nHTML(\"\"\"<style> \n            .output_png { display: table-cell; text-align: center; vertical-align: middle; } \n     </style> \"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Importing the Data using pandas read_csv() and we will drop the columns id, url and created as I am not gonna be using these columns for analysis**","metadata":{}},{"cell_type":"code","source":"reddit = pd.read_csv('../input/reddit-wallstreetsbets-posts/reddit_wsb.csv')\nreddit.drop(columns=['id', 'url', 'created'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calling head() and info() in the DataFrame**","metadata":{}},{"cell_type":"code","source":"reddit.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above information on the the DataFrame we can clearly see that we only have NaN in the case of body and also around half the cells in the Body Column are NaN.","metadata":{}},{"cell_type":"markdown","source":"---\n**Which Day of the week we have most Post**\n\n1. We are converting the 'timestamp' to a datetime object\n2. Using the weekday() from the datetime object we get the Day of the Week\n3. We will plot a barplot using the Seaborn and set the order of days accordingly","metadata":{}},{"cell_type":"code","source":"reddit['timestamp'] = pd.to_datetime(reddit['timestamp'])\n\nday_of_the_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\ndays_order = list(day_of_the_week.values())\n\nreddit['Weekday'] = reddit['timestamp'].apply(lambda x : day_of_the_week[x.weekday()])\n\nxs = reddit['Weekday'].value_counts().index\nys = reddit['Weekday'].value_counts().values\n\nplt.figure(figsize=(14,6))\n\nsns.barplot(x=xs, y=ys, order=days_order)\n\nplt.title(\"No. of Posts vs Day of the Week\", fontsize=15)\n\nplt.xlabel(\"Days\", fontsize=15)\nplt.ylabel(\"No. of Posts\", fontsize=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above Barplot we can clearly see that the there were huge number of posts on Friday.","metadata":{}},{"cell_type":"markdown","source":"---\n**Focusing more on the 'title' and 'body' column of the data**\n\n1. We will preprocess the Text Data in the Title and Body using the clean_text_date().\n2. The function will remove Handlers, URLs, Special Characters, Single Characters and Extra Spaces","metadata":{}},{"cell_type":"code","source":"reddit_title = reddit['title'].dropna()\nreddit_body = reddit['body'].dropna()\n\n\ndef clean_text_date(text):\n    text = text.lower()\n\n    # Replacing Handlers with Empty String\n    text = re.sub('@[^\\s]+','',text)\n\n    # Replacing URLs with Empty String\n    text = re.sub(r\"http\\S+\", \"\",text)\n\n    # Remove all the special characters\n    text = ' '.join(re.findall(r'\\w+', text))\n\n    # Replacing Single Characters with Empty String\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n\n    # Removing Extra Spaces\n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    \n    return text\n\n    \n# Text Preprocessing\nreddit_title = reddit_title.apply(lambda x : clean_text_date(x))\nreddit_body = reddit_body.apply(lambda x : clean_text_date(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n**Plotting a Histograms to see the Length(No. of words) Distribution of Title and Body**\n\n1. We use the word_tokenize from nltk on each Title and Body to get a list of Lengths.\n2. Then using the Seaborn histplot we plot a histogram. ","metadata":{}},{"cell_type":"code","source":"title_length = [len(word_tokenize(text)) for text in reddit_title]\nbody_length = [len(word_tokenize(text)) for text in reddit_body]\n\nfig, (axis1, axis2) = plt.subplots(1,2, figsize=(16,6))\n\nsns.histplot(title_length, bins=50, kde=True, ax=axis1)\nsns.histplot(body_length, bins=40, kde=True, ax=axis2)\n\naxis1.set_xlabel(\"Length of Title\")\naxis2.set_xlabel(\"Length of Body\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n**WordCloud for Title and Body of the Post**","metadata":{}},{"cell_type":"code","source":"word_tokens = [word_tokenize(text) for text in reddit_title]\n\nword_cloud_string = \"\"\n\nfor word_list in word_tokens:\n    for word in word_list:\n        if word.lower() in english_words:\n            word_cloud_string += word + \" \"\n        \n# Updating some of the Words into Stopwords \ndescription_stopwords = set(STOPWORDS)\n\nmy_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\nplt.figure(figsize=(10,20))\nplt.imshow(my_word_cloud, interpolation='bilinear')\nplt.title(\"Word Cloud for Post Title\", fontsize=20)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_tokens = [word_tokenize(text) for text in reddit_body]\n\nword_cloud_string = \"\"\n\nfor word_list in word_tokens:\n    for word in word_list:\n        if word.lower() in english_words:\n            word_cloud_string += word + \" \"\n        \n# Updating some of the Words into Stopwords \ndescription_stopwords = set(STOPWORDS)\n\nmy_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\nplt.figure(figsize=(10,20))\nplt.imshow(my_word_cloud, interpolation='bilinear')\nplt.title(\"Word Cloud for Post Body\", fontsize=20)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n**Sentiment Analysis Using SentimentIntensityAnalyzer from nltk.sentiment**","metadata":{}},{"cell_type":"markdown","source":"1. Calling get_sentiment() on each Title and Body\n2. Creating a new Column called 'Sentiment' and storing the respective sentiment\n3. Using the Sentiment column to plot various graphs.","metadata":{}},{"cell_type":"code","source":"def get_sentiment(sia, text):\n    if sia.polarity_scores(text)[\"compound\"] > 0:\n        return \"Positive\"\n    elif sia.polarity_scores(text)[\"compound\"] < 0:\n        return \"Negative\"\n    else:\n        return \"Neutral\"\n\nsia = SentimentIntensityAnalyzer()    \n    \nreddit_title_df = reddit_title.to_frame(name='Title')\nreddit_title_df['Sentiment'] = reddit_title_df['Title'].apply(lambda x : get_sentiment(sia, x))\n\nreddit_body_df = reddit_body.to_frame(name='Body')\nreddit_body_df['Sentiment'] = reddit_body_df['Body'].apply(lambda x : get_sentiment(sia, x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting a Bar Graph for Sentiment Counts**","metadata":{}},{"cell_type":"code","source":"fig, (axis1, axis2) = plt.subplots(1,2, figsize=(12,5))\n\norder = ['Positive', 'Neutral', 'Negative']\n\nxs = reddit_title_df['Sentiment'].value_counts().index\nys = reddit_title_df['Sentiment'].value_counts().values\nsns.barplot(x=xs, y=ys, order=order, ax=axis1)\n\nxs = reddit_body_df['Sentiment'].value_counts().index\nys = reddit_body_df['Sentiment'].value_counts().values\nsns.barplot(x=xs, y=ys, order=order, ax=axis2)\n\naxis1.set_title(\"For Title\")\naxis2.set_title(\"For Body\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n**WordCloud for Different Sentiment**","metadata":{}},{"cell_type":"markdown","source":"1. Title","metadata":{}},{"cell_type":"code","source":"fig, (axis1, axis2, axis3) = plt.subplots(3, 1, figsize=(12,18))\n\naxes = [axis1, axis2, axis3]\nsentiments = ['Positive', 'Neutral', 'Negative']\n\nfor i in range(3):\n    word_tokens = [word_tokenize(text) for text in reddit_title_df[reddit_title_df['Sentiment'] == sentiments[i]]['Title']]\n    \n    word_cloud_string = \"\"\n    \n    for word_list in word_tokens:\n        for word in word_list:\n            if word.lower() in english_words:\n                word_cloud_string += word + \" \"\n    \n    description_stopwords = set(STOPWORDS)\n\n    my_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\n    axes[i].imshow(my_word_cloud, interpolation='bilinear')\n    axes[i].set_title(f\"Word Cloud for Post Title with {sentiments[i]} Sentiment\", fontsize=20)\n    axes[i].axis('off')\n\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n2. Body","metadata":{}},{"cell_type":"code","source":"fig, (axis1, axis2, axis3) = plt.subplots(3, 1, figsize=(12,18))\n\naxes = [axis1, axis2, axis3]\nsentiments = ['Positive', 'Neutral', 'Negative']\n\nfor i in range(3):\n    word_tokens = [word_tokenize(text) for text in reddit_body_df[reddit_body_df['Sentiment'] == sentiments[i]]['Body']]\n    \n    word_cloud_string = \"\"\n    \n    for word_list in word_tokens:\n        for word in word_list:\n            if word.lower() in english_words:\n                word_cloud_string += word + \" \"\n    \n    description_stopwords = set(STOPWORDS)\n\n    my_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\n    axes[i].imshow(my_word_cloud, interpolation='bilinear')\n    axes[i].set_title(f\"Word Cloud for Post Body with {sentiments[i]} Sentiment\", fontsize=20)\n    axes[i].axis('off')\n\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Thank You","metadata":{}}]}